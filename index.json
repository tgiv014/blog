[{"content":"Mag Field Inspired by E/H-field diagrams and built using numpy and pycairo. This project draws a visually pleasing representation of the magnetic field created by randomly placed infinite current-carrying wires extending out of the canvas.\nCircle Packer This project draws something pretty similar to the color blindness test everyone knows and loves. Instead of packing circles into a larger circle, it packs them randomly into a mask rendered from an input string. It can also output images as SVG files suitable for use with a plotter or laser cutter.\n","permalink":"https://mntn.dev/projects/generativeart/","summary":"Programs that make pretty pictures.","title":"Generative Art"},{"content":"Shortly after opening up my home server\u0026rsquo;s ssh port to the world (with only public/private key auth enabled), I started getting a flood of attempted ssh requests from all over the place. Out of curiosity, I wanted to see them plotted out on my Grafana dashboard so I started putting together a small service that reads SSH authentication attempts, parses them, and pushes them into influxdb.\nThis project originally started out as my first foray into Rust. Pretty soon after that, I began experimenting with Golang for systems programming and decided to port what I had made so far into Go. Links for both versions below:\n Authmap (Rust) (Not maintained) Authmap (Go)  Goals I have a few goals to refine this project into something more useful:\n Remove every instance of hardcoding and read all configurables from a YAML file.  Done! Now, the only hardcoded configuration item is the config\u0026rsquo;s location. The application will also generate a default configuration automatically if a config is not found.   Allow grouping requests from the same IP/location in a smart way.  Done! The ip field is now a tag. This allows grouping by ip and aggregating results within those groups.   Automate downloading the MaxMind GeoIP database.  This will require user interaction since the database has licensing requirements now.   Make the docker container for this package more accessible.  Done! Authmap (go) is now available from docker hub as tgiv014/authmap:latest. See the Authmap repo for installation instructions and an example docker-compose configuration    ","permalink":"https://mntn.dev/projects/authmap/","summary":"A service that geolocates unwanted SSH attempts.","title":"Authmap"},{"content":"This is an addendum to this previous post.\nIt\u0026rsquo;s learnin' time While completing advent of code 2020, I learned about memoization. I realized that this is a perfect problem to apply the technique to. All we need is a hashmap indexed by our tree coordinates that we\u0026rsquo;ll pass in our recursive function. If there\u0026rsquo;s a cached value for the function inputs, we early return the cached value. In our normal return path we save off the calculated value before actually returning it. It\u0026rsquo;s a simple formula for faster processing.\nWithout further ado:\nuse std::collections::HashMap; #[derive(Hash, Eq, PartialEq, Debug)] struct Coordinate { x: u32, y: u32, } // The size of our domain in steps // The grid is NxN steps // Really, our position grid is N+1xN+1 // [0,0] describes the start and [N,N] describes the end point static N:u32 = 20; fn build_node(x:u32, y:u32, cache:\u0026amp;mut HashMap\u0026lt;Coordinate, u64\u0026gt;) -\u0026gt; u64 { let c = Coordinate{x,y}; let mut sum:u64 = 0; if cache.contains_key(\u0026amp;c) { return cache[\u0026amp;c]; } // If x==y==N, we\u0026#39;re at the end point!  if x == N \u0026amp;\u0026amp; y == N { return 1; } // Recurse for each child node if possible  if x \u0026lt; N { sum += build_node(x + 1, y, cache); } if y \u0026lt; N { sum += build_node(x, y + 1, cache); } cache.insert(c, sum); return sum; } fn main() { let mut cache:HashMap\u0026lt;Coordinate, u64\u0026gt; = HashMap::new(); let n_ends:u64 = build_node(0, 0, \u0026amp;mut cache); println!(\u0026#34;Total unique paths: {}\u0026#34;, n_ends); } Now it takes ~5ms to evaluate every path in a 20x20 grid! That\u0026rsquo;s a heck of a lot better than 30min.\n","permalink":"https://mntn.dev/posts/3-project-euler-15-add/","summary":"Showing a \u003cem\u003ereally\u003c/em\u003e obvious project euler answer that I didn\u0026rsquo;t see before.","title":"Project Euler Problem #15 - Addendum"},{"content":"The Problem Lately, I\u0026rsquo;ve been playing with algorithmic trading using Backtrader, which is a spectacular tool for experimenting with algorithmic trading strategies. This hobby could be considered a problem in itself, but that\u0026rsquo;s not today\u0026rsquo;s topic. A strategy I\u0026rsquo;ve been testing involves maintaining a list of S\u0026amp;P500 stocks, ranking them by some metrics, and making a portfolio out of the top 20% or so. Essentially, a really impersonal buy-and-hold strategy.\nUnfortunately, this means I need to collect data on 500 stocks over my backtesting time range (5+ years). I would really prefer to use Backtrader\u0026rsquo;s automatic Yahoo Finance data source bt.feeds.YahooFinanceData, but I don\u0026rsquo;t want to send 500+ requests to Yahoo every time I tweak a single variable. On a list of just 16 stocks, it takes ~40 seconds to query 5 years of data and run the strategy. There has to be a better way.\nEnter requests-cache! Backtrader supports proxies (so we could set up a caching proxy), but under the hood, backtrader\u0026rsquo;s Yahoo feed uses the requests module! This means we can solve our problem without even touching a proxy service or having to set up local SSL certificates to cache HTTPS.\nAll we need to do is run pip install requests-cache and add two lines of code to the start of our Backtrader script.\nimport requests_cache requests_cache.install_cache(\u0026#39;test_cache\u0026#39;, expire_after=3600) Now, requests-cache will automatically cache any requests made via the python request module in a file called test_cache.sqlite. Even HTTPS. With this little change, backtesting against 16 stocks over 5 years finishes in 10 seconds: an improvement of 75%!\n","permalink":"https://mntn.dev/posts/2-requests-cache/","summary":"Caching uber-repetitive Yahoo Finance queries","title":"Faster Backtesting with requests-cache"},{"content":"Starting in 2020, I\u0026rsquo;ve made it a personal goal to complete Advent of Code in a new language every year. In 2020, I kicked this tradition off with Golang.\n2020 This year I decided to pick up Go after watching a free conference hosted by the folks at Dgraph. This was an awesome way to stretch my legs with Go and get comfortable with the syntax and built-in data structures of Go. All my solutions are on my Github.\nNoteworthy Puzzles  Day 10  The first part of the puzzle was easy to solve with built-in sorting functions and iteration, but the second part convinced me to build a graph of every adapter and the other adapters they are compatible with. This solution worked, but desperately needed optimization. In came memoization (a topic that never came up in my EE curriculum).   Day 20  This was a fun puzzle, but it was also mean. It essentially required you to implement transforms (mirrors and rotations) on 2d boolean arrays and also correlate the edges of the arrays with each other to figure out their position and orientation in a larger image. It took quite a bit of pen and paper sketching to come up with a sensible coordinate system and accurate transforms.    ","permalink":"https://mntn.dev/projects/advent/advent/","summary":"My Advent of Code Solutions","title":"Advent of Code"},{"content":"Intro Problem #15 from Project Euler is a really interesting problem. Not only is there a clean mathematical solution to the question, but there are a bunch of non-optimal (or even downright ugly) but intuitive ways to solve the problem.\nThe Easy Way The easiest, and probably fastest, way to solve this problem is to just use math. Consider the rules of the problem:\n Each path is composed of 2N steps that lead from the top left to the bottom right of the domain. Each step can only be a rightward or downward move. Because of the start and end positions, each path must have an equal number of down and right moves.  Based on these rules, you can build a valid path by starting with an empty list of 2N steps, selecting N steps to place a rightward move, and filling the remaining steps with downward moves. This is a Combination Problem! The number of valid paths you can possibly construct is 2N choose N.\nA Fun Way   Maybe you\u0026rsquo;re itching for a reason to think about binary trees. You can look at every step in a path as a node in a binary tree. The starting position is the root node. The root node has two child nodes, one for a downward move and one for a rightward move. Each of those nodes have their own children in the same pattern. This means that two of the \u0026ldquo;grandchild\u0026rdquo; nodes actually describe different paths to the same point. In the above picture, downward moves are to the left and rightward to the right.\nUsing this tree, we can describe any path on an infinite grid. To constrain the grid (and know when we\u0026rsquo;ve reached the end point), we need to add some state to each node: an x and y position: (x,y). To create a new downward node in our domain, y must be less than N - the new node\u0026rsquo;s y value will be increased by 1. To create a new rightward node, x must be less than N - we\u0026rsquo;ll increase the node\u0026rsquo;s x. If a new node has state x == N and y == N, we\u0026rsquo;ve hit the end point of the grid!\nIf we actually build out this tree to full 2N depth, we\u0026rsquo;ll eat up a ton of memory. Conveniently, we don\u0026rsquo;t actually need to hold all of this state since we\u0026rsquo;re only interested in how many nodes in the tree are at the end point. We can just pretend to build the tree with a recursive function. Here\u0026rsquo;s an implementation in RustðŸ¦€:\n// The size of our domain in steps // The grid is NxN steps // Really, our position grid is N+1xN+1 // [0,0] describes the start and [N,N] describes the end point static N:u32 = 20; fn build_node(x:u32, y:u32, n_ends:\u0026amp;mut u64) { // If x==y==N, we\u0026#39;re at the end point!  if x == N \u0026amp;\u0026amp; y == N { *n_ends += 1; return; } // Recurse for each child node if possible  if x \u0026lt; N { build_node(x + 1, y, n_ends); } if y \u0026lt; N { build_node(x, y + 1, n_ends); } } fn main() { let mut n_ends:u64 = 0; // This describes the root node  build_node(0, 0, \u0026amp;mut n_ends); println!(\u0026#34;Total unique paths: {}\u0026#34;, n_ends); } Go get a coffee or two, because this took ~30min to run on my machine.\nEdit: This can be heavily optimized without abandoning the tree structure! More info in this more recent post.\nAn ugly way Suppose you were told to solve this problem on an FPGA and you have no clue what combination is. Since we know the length of a path is 2N, we\u0026rsquo;ll start with a 2N-bit wide counter. Let\u0026rsquo;s say down = 1 and right = 0. We can initialize a down-counter with all 1s and let it run all the way down to 0. Since we know a valid path has #rights=#downs=N, we\u0026rsquo;ll look at every counter value and keep track of the number of values which have exactly N ones. That number is the number of possible paths!\nWe can even slightly optimize this by recognizing that we only have to evalute all of the paths that start with a downward move and multiply the result by two. This follows from our tree representation above. Rust example below:\n// The size of our domain in steps static N:u32 = 20; fn main() { let n_ends:u64 = (2u64.pow(2*N-1)..2u64.pow(2*N)).map(|x| (x.count_ones()==N) as u64).sum(); println!(\u0026#34;Total unique paths: {}\u0026#34;, 2*n_ends); } Go get another coffee. This took ~15 minutes on my machine. In the end, the mathematical solution is the way to go, but it\u0026rsquo;s still fun to explore other solutions.\n","permalink":"https://mntn.dev/posts/1-project-euler-15/","summary":"Three ways to tackle Project Euler Problem #15","title":"Project Euler Problem #15"},{"content":"  How\u0026rsquo;d we Get Here\u0026hellip; A while back, I snagged a Dell R710 from Ebay for $350 delivered. This thing is a virtualization monster. 8 cores and 144GB of ECC RAM, perfect for filling with bhyve virtual machines on FreeNAS. This setup worked great for me until I became interested in Docker containers. FreeNAS just doesn\u0026rsquo;t support a docker environment, so let\u0026rsquo;s build something from the ground up and document the process.\nConstraints  Must use ZFS (no need for ZFS root) Native docker support Lightweight (no GUI, unnecessary package systems *cough cough snaps*, etc.)  With these constraints, we know we absolutely must use Linux and we would really like a distro that has openzfs in its package control. In the past I\u0026rsquo;ve used Ubuntu Server and ubuntu 16.04 has zfs available as a package, but I want to avoid snaps like the plague. Because of this, I\u0026rsquo;m going to use Debian.\nInitial Setup First things first, we\u0026rsquo;re going to shut down the server and remove every drive but our desired boot drive. FreeNAS uses a flash drive as its root storage, but debian doesn\u0026rsquo;t quite support that. In my case, I\u0026rsquo;m going to use an old 120GB SSD in an optical drive caddy in place of the server\u0026rsquo;s disk drive. It\u0026rsquo;s easy enough to complete the guided install, and Debian has some pretty good installation literature:\n Installation Guide - Long Installation Howto - Short  I do have a few gotchas during the guided install:\n Some NICs require closed-source firmware to operate. The NIC in my R710 is one of those, so I had to use the non-free debian image. Guided partitioning failed for my install. I suspect that this is because it was trying to make a gigantic swap partition to match the 144GB of RAM. I decided not to use a swap partition and partition the entire SSD as /. I avoided giving root a password. Instead, I have an administrator user with sudo access. I did not install any desktop environments or X server components. Don\u0026rsquo;t forget to select openssh!  Post-Install Now that we have a fully running debian system, let\u0026rsquo;s change a few things. Pop all your drives back in and ssh in.\nLet\u0026rsquo;s gear up to use ssh without a password, starting by setting up private key authentication. If you\u0026rsquo;re on linux, you\u0026rsquo;ve got it easy: $ ssh-copy-id $USER@$HOST. On Windows, I find it easiest to manually copy the contents of %HOMEPATH%\\.ssh\\id_rsa.pub to ~/.ssh/authorized_keys on my server. If you don\u0026rsquo;t have an ~/.ssh/id_rsa.pub or %HOMEPATH%\\.ssh\\id_rsa.pub, you better generate an ssh key with $ ssh-keygen.\nOnce your public key is copied in, it\u0026rsquo;s a good idea to exit your ssh session and attempt to login with private key auth. If you don\u0026rsquo;t need a password, you\u0026rsquo;re on the right path. Time to disable password authentication. In /etc/ssh/sshd_config, find the lines containing the following keys, uncomment them and make sure they\u0026rsquo;re set to no.\nPasswordAuthentication no ChallengeResponseAuthentication no PermitRootLogin no UsePAM no Now you can reload sshd and bask in paswordless security strong enough to leave exposed publicly. $ sudo /etc/init.d/sshd reload\nZFS Time Debian makes this one super easy. Check the official ZFS Debian Wiki Page for more info. Use the following commands to install ZFS on Debian.\n$ sudo apt update $ sudo apt install linux-headers-`uname -r` $ sudo apt install -t buster-backports dkms sol-dkms $ sudo apt install -t buster-backports zfs-dkms zfsutils-linux It may take a little while to run the dkms build, and it may throw a few warnings. They do not specifically recommend a reboot on the Debian Wiki, but it\u0026rsquo;s never a bad idea when dealing with kernel modules.\nJust for grins, if you happen to be installing on a system that used to run FreeNAS, you can try to import your old pool with $ sudo zpool import. You\u0026rsquo;re likely to see action: The pool can be imported using its name or numeric identifier and the '-f' flag.. You can force the import by running $ sudo zpool import -f $POOLNAME.\nIf you\u0026rsquo;re starting fresh, now is the time to make your first zfs pool. I\u0026rsquo;m in a bit of a hard drive shortage, so for testing\u0026rsquo;s sake I will be doing a stripe pool with two 500GB hard drives. ZFS uses disk IDs or paths to reference disks instead of /dev/sdX paths. You can figure out which disk is which with the following commands:\n$ ls -l /dev/disk/by-id $ ls -l /dev/disk/by-path $ lsblk -o NAME,SIZE,MODEL,VENDOR With the device IDs in hand, I\u0026rsquo;ll make a zpool with:\n$ sudo zpool create tank ata-ST9500325AS_6VESPM0A ata-ST9500325AS_6VESRNXY Docker Install This is essentially a summary of the Docker Docs for Debian.\n Update apt and install required packages:  $ sudo apt update $ sudo apt install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common Get Docker\u0026rsquo;s GPG key:  $ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - You can verify you have the correct key as follows:\n$ sudo apt-key fingerprint 0EBFCD88 pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid Docker Release (CE deb) \u0026lt;docker@docker.com\u0026gt; sub 4096R/F273FCD8 2017-02-22 Use the following command to set up Docker stable.  $ sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/debian \\ $(lsb_release -cs) \\ stable\u0026quot; Finally, install docker!  $ sudo apt update $ sudo apt install docker-ce docker-ce-cli containerd-io If you want to allow other users to run docker commands, you can set that up as follows:  $ sudo groupadd docker $ sudo usermod -aG docker $USER Log out and back in\n You can verify that your docker install is functional by running hello-world  $ docker run --rm hello-world Let\u0026rsquo;s Hook up Docker and ZFS This part\u0026rsquo;s for the adventurous. You could certainly just make a bunch of ZFS datasets with filesystem mounts and mount those in your containers for bulk storage, but that\u0026rsquo;s no fun! Docker has a ZFS storage driver that will back all container, image, and volume storage with zfs. Let\u0026rsquo;s set it up.\n Kill docker:  $ sudo /etc/init.d/docker stop Back up /var/lib/docker just in case and then delete its contents:  $ sudo cp -au /var/lib/docker /var/lib/docker.bk $ sudo rm -rf /var/lib/docker/* Create a ZFS-backed mountpoint at /var/lib/docker. The official docker instructions suggest making a zpool and mounting it there, but I\u0026rsquo;ll use a dataset since I already have a zpool configured (name tank).  $ sudo zfs create -o mountpoint=/var/lib/docker tank/docker Tell Docker to use ZFS for its storage driver. In /etc/docker/daemon.json:  { \u0026quot;storage-driver\u0026quot;: \u0026quot;zfs\u0026quot; } Start docker back up and verify you\u0026rsquo;re using the ZFS storage driver:  $ sudo /etc/init.d/docker start $ docker info Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 114 Server Version: 19.03.11 Storage Driver: zfs Zpool: tank Zpool Health: ONLINE Parent Dataset: tank/docker Space Used By Parent: 17283463680 Space Available: 935802788864 Parent Quota: no Compression: off Logging Driver: json-file Cgroup Driver: cgroupfs ðŸŽ‰ Done! Now we have a server running Debian with Docker using ZFS-backed storage. This would be a great way to put together a NAS setup using samba or NFS with a bunch of self-hosted services. I\u0026rsquo;ll be using Docker to host a home assistant instance with nginx sitting in front as a reverse proxy.\n","permalink":"https://mntn.dev/posts/4-docker-zfs-deb-1/","summary":"A look at the setup of my home server.","title":"Docker \u0026 ZFS On Debian"}]